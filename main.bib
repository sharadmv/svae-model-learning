@article{Adams2008,
abstract = {We propose the Gaussian Process Density Sampler (GPDS), a nonparametric, practical and consistent method of constructing a Markov chain on the properties of a posterior distribution on an unknown density, without approximation.},
author = {Adams, Ryan and Murray, Iain and MacKay, David},
journal = {Advances in Neural Information Processing Systems},
keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
pages = {1--8},
title = {{The Gaussian Process Density Sampler}},
url = {http://eprints.pascal-network.org/archive/00004284/},
volume = {21},
year = {2008}
}
@article{Adams2010,
abstract = {Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a flexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are infinitely exchangeable. One can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data.},
archivePrefix = {arXiv},
arxivId = {1006.1062},
author = {Adams, Ryan Prescott and Ghahramani, Zoubin and Jordan, Michael I.},
eprint = {1006.1062},
file = {:home/sharad/Documents/Mendeley Desktop/Adams, Ghahramani, Jordan - 2010 - Tree-Structured Stick Breaking Processes for Hierarchical Data.pdf:pdf},
isbn = {9781617823800},
journal = {Advances in Neural {\ldots}},
keywords = {Computational, Information-Theoretic Learning with,Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics,Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
pages = {16},
title = {{Tree-Structured Stick Breaking Processes for Hierarchical Data}},
url = {http://arxiv.org/abs/1006.1062},
year = {2010}
}
@article{Aho1981,
abstract = {We present an algorithm for constructing a tree to satisfy a set of lineage constraints on common ancestors. We then apply this algorithm to synthesize a relational algebra expression from a simple tableau, a problem arising in the theory of relational databases.},
author = {Aho, A. V. and Sagiv, Y. and Szymanski, T. G. and Ullman, J. D.},
doi = {10.1137/0210030},
file = {:home/sharad/Documents/Mendeley Desktop/aho-sag-1981.pdf:pdf},
issn = {0097-5397},
journal = {SIAM Journal on Computing},
keywords = {join minimization,lowest common ancestors,query optimization,relational algebra,relational databases,tableaux,tree algorithms},
language = {en},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics,Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Hierarchical Clustering},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Inferring a Tree from Lowest Common Ancestors with an Application to the Optimization of Relational Expressions}},
url = {http://epubs.siam.org/doi/abs/10.1137/0210030},
year = {1981}
}
@inbook{Aldous1985,
address = {Berlin, Heidelberg},
author = {Aldous, David J.},
chapter = {Exchangeability and related topics},
doi = {10.1007/BFb0099421},
editor = {Hennequin, P L},
isbn = {978-3-540-39316-0},
pages = {1--198},
publisher = {Springer Berlin Heidelberg},
title = {{{\'{E}}cole d'{\'{E}}t{\'{e}} de Probabilit{\'{e}}s de Saint-Flour XIII --- 1983}},
url = {http://dx.doi.org/10.1007/BFb0099421},
year = {1985}
}
@book{Aldous1996,
address = {New York, NY},
doi = {10.1007/978-1-4612-0719-1},
author = {Aldous, David J.},
isbn = {978-1-4612-6881-9},
publisher = {Springer New York},
series = {The IMA Volumes in Mathematics and its Applications},
title = {{Random Discrete Structures}},
url = {http://www.springerlink.com/index/10.1007/978-1-4612-0719-1},
year = {1996}
}
@article{Aldous1999,
author = {Aldous, David J.},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {Smoluchowski coagulation equation,branching process,coalescence,continuum tree,density-dependent Markov process,gelation,random graph,random tree},
month = {feb},
number = {1},
pages = {3--48},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
title = {{Deterministic and stochastic models for coalescence (aggregation and coagulation): a review of the mean-field theory for probabilists}},
url = {http://projecteuclid.org/euclid.bj/1173707093},
volume = {5},
year = {1999}
}
@article{Aldous2000,
abstract = {A cladogram is a tree with labelled leaves and unlabelled degree-3 branchpoints. A certain Markov chain on the set of n- leaf cladograms consists of removing a random leaf (and its incident edge) and re-attaching it to a random edge. We show that the mixing time (time to approach the uniform stationary distribution) for this chain is at least O(n(2)) and at most O(n(3)).},
author = {Aldous, David J.},
journal = {Combinatorics Probability {\&} Computing},
keywords = {Markov chain,Markov chain Monte Carlo,coupling,mixing time,phylogenetic tree},
number = {3},
pages = {191--204},
title = {{Mixing time for a Markov chain on cladograms}},
volume = {9},
year = {2000}
}
@article{Ankerst1999,
abstract = {Cluster analysis is a primary method for database mining. It is either used as a stand-alone tool to get insight into the distribution of a data set, e.g. to focus further analysis and data processing, or as a preprocessing step for other algorithms operating on the detected clusters. Almost all of the well-known clustering algorithms require input parameters which are hard to determine but have a significant influence on the clustering result. Furthermore, for many real-data sets there does not even exist a global parameter setting for which the result of the clustering algorithm describes the intrinsic cluster- ing structure accurately. We introduce a new algorithm for the pur- pose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of param- eter settings. It is a versatile basis for both automatic and interactive cluster analysis. We show how to automatically and efficiently extract not only â€˜traditional' clustering information (e.g. representa- tive points, arbitrary shaped clusters), but also the intrinsic cluster- ing structure. For medium sized data sets, the cluster-ordering can be represented graphically and for very large data sets, we introduce an appropriate visualization technique. Both are suitable for inter- active exploration of the intrinsic clustering structure offering addi- tional insights into the distribution and correlation of the data.},
author = {Ankerst, Mihael and Breunig, Markus M. and Kriegel, Hans-Peter and Sander, J{\"{o}}rg},
journal = {ACM Sigmod Record},
keywords = {cluster analysis,database mining,visualization},
pages = {49--60},
title = {{Optics: Ordering points to identify the clustering structure}},
url = {http://dl.acm.org/citation.cfm?id=304187},
year = {1999}
}
@article{Blei2010,
abstract = {We present the nested Chinese restaurant process (nCRP), a stochastic process which assigns probability distributions to infinitely-deep, infinitely-branching trees. We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections. Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction. Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree. We demonstrate this algorithm on collections of scientific abstracts from several journals. This model exemplifies a recent trend in statistical machine learning--the use of Bayesian nonparametric methods to infer distributions on flexible data structures.},
archivePrefix = {arXiv},
arxivId = {0710.0845},
author = {Blei, David M. and Griffiths, Thomas L. and Jordan, Michael I.},
doi = {10.1145/1667053.1667056},
eprint = {0710.0845},
file = {:home/sharad/Documents/Mendeley Desktop/Blei, Griffiths, Jordan - 2010 - The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies.pdf:pdf},
isbn = {00045411},
issn = {00045411},
journal = {Journal of the ACM},
keywords = {Bayesian nonparametric statistics,unsupervised learning},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics,Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
month = {jan},
number = {2},
pages = {1--30},
publisher = {ACM},
title = {{The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies}},
url = {http://arxiv.org/abs/0710.0845},
volume = {57},
year = {2010}
}
@article{Boyles2012,
author = {Boyles, Levi and Welling, Max},
file = {:home/sharad/Documents/Mendeley Desktop/Boyles, Welling - 2012 - The time-marginalized coalescent prior for hierarchical clustering.pdf:pdf},
journal = {Advances in Neural Information Processing {\ldots}},
keywords = {TMC},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
mendeley-tags = {TMC},
title = {{The time-marginalized coalescent prior for hierarchical clustering}},
url = {http://papers.nips.cc/paper/4786-the-time-marginalized-coalescent-prior-for-hierarchical-clustering},
year = {2012}
}
@article{Dempster1977,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
archivePrefix = {arXiv},
arxivId = {0710.5696v2},
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
doi = {10.1.1.133.4884},
eprint = {0710.5696v2},
isbn = {0000000779},
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society, Series B},
pmid = {9501024},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.133.4884},
year = {1977}
}

@article{Evans2006,
abstract = {We use Dirichlet form methods to construct and analyze a reversible Markov process, the stationary distribution of which is the Brownian continuum random tree. This process is inspired by the subtree prune and regraft (SPR) Markov chains that appear in phylogenetic analysis. A key technical ingredient in this work is the use of a novel Gromov--Hausdorff type distance to metrize the space whose elements are compact real trees equipped with a probability measure. Also, the investigation of the Dirichlet form hinges on a new path decomposition of the Brownian excursion.},
archivePrefix = {arXiv},
arxivId = {math/0502226},
author = {Evans, Steven N. and Winter, Anita},
doi = {10.1214/009117906000000034},
eprint = {0502226},
file = {:home/sharad/Documents/Mendeley Desktop/Steven N. Evans - Unknown - Subtree prune and re-graft a reversible real tree valued markov process.pdf:pdf},
issn = {0091-1798},
journal = {The Annals of Probability},
keywords = {SPR},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
mendeley-tags = {SPR},
month = {may},
number = {3},
pages = {918--961},
primaryClass = {math},
title = {{Subtree prune and regraft: A reversible real tree-valued Markov process}},
url = {http://arxiv.org/abs/math/0502226},
volume = {34},
year = {2006}
}
@article{Ester1996,
abstract = {Data clustering has become an important task for discovering significant patterns and characteristics in large spatial databases. The Multi-Centroid, Multi-Run Sampling Scheme (MCMRS) has been shown to be effective in improving the k-medoids-based clustering algorithms in our previous work. In this paper, a more advanced sampling scheme termed the Incremental (IMCMRS) is proposed for k-medoids-based clustering algorithms. Experimental results demonstrate the proposed scheme can not only reduce...},
author = {Ester, Martin and Kriegel, Hans P and Sander, Jorg and Xu, Xiaowei},
journal = {Second International Conference on Knowledge Discovery and Data Mining},
keywords = {arbitrary shape of clus-,clustering algorithms,databases,efficiency on large spatial,handling noise,ters},
pages = {226--231},
title = {{A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.2930},
year = {1996}
}
@article{Hagen1992,
abstract = {Partitioning of circuit netlists in VLSI design is considered. It is shown that the second smallest eigenvalue of a matrix derived from the netlist gives a provably good approximation of the optimal ratio cut partition cost. It is also demonstrated that fast Lanczos-type methods for the sparse symmetric eigenvalue problem are a robust basis for computing heuristic ratio cuts based on the eigenvector of this second eigenvalue. Effective clustering methods are an immediate by-product of the second eigenvector computation and are very successful on the difficult input classes proposed in the CAD literature. The intersection graph representation of the circuit netlist is considered, as a basis for partitioning, a heuristic based on spectral ratio cut partitioning of the netlist intersection graph is proposed. The partitioning heuristics were tested on industry benchmark suites, and the results were good in terms of both solution quality and runtime. Several types of algorithmic speedups and directions for future work are discussed},
author = {Hagen, Lars and Kahng, Andrew B.},
journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
number = {9},
pages = {1074--1085},
title = {{New Spectral Methods for Ratio Cut Partitioning and Clustering}},
volume = {11},
year = {1992}
}
@article{Harding1971,
abstract = {The set of rooted trees, generated by random bifurcation at the terminal nodes, is considered with the aims of enumerating it and of determining its probability distribution. The account of enumeration collates much previous work and attempts a complete perspective of the problems and their solutions. Asymptotic and numerical results are given, and some unsolved problems are pointed out. The problem of ascertaining the probability distribution is solved by obtaining its governing recurrence equation, and numerical results are given. The difficult problem of determining the most probable tree-shape of given size is considered, and for labelled trees a conjecture at its solution is offered. For unlabelled shapes the problem remains open. These mathematical problems arise in attempting to reconstruct evolutionary trees by the statistical approach of Cavalli-Sforza and Edwards. CR - Copyright {\&}{\#}169; 1971 Applied Probability Trust},
author = {Harding, E. F.},
journal = {Advances in Applied Probability},
publisher = {Applied Probability Trust},
title = {{The Probabilities of Rooted Tree-Shapes Generated by Random Bifurcation}},
url = {http://www.jstor.org/stable/1426329},
year = {1971}
}
@book{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
doi = {10.1007/b94608},
isbn = {9780387848570},
issn = {03436993},
pmid = {15512507},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/10.1007/b94608},
year = {2009}
}
@article{Hordijk2005,
abstract = {MOTIVATION: Maximum likelihood (ML) methods have become very popular for constructing phylogenetic trees from sequence data. However, despite noticeable recent progress, with large and difficult datasets (e.g. multiple genes with conflicting signals) current ML programs still require huge computing time and can become trapped in bad local optima of the likelihood function. When this occurs, the resulting trees may still show some of the defects (e.g. long branch attraction) of starting trees obtained using fast distance or parsimony programs.

METHODS: Subtree pruning and regrafting (SPR) topological rearrangements are usually sufficient to intensively search the tree space. Here, we propose two new methods to make SPR moves more efficient. The first method uses a fast distance-based approach to detect the least promising candidate SPR moves, which are then simply discarded. The second method locally estimates the change in likelihood for any remaining potential SPRs, as opposed to globally evaluating the entire tree for each possible move. These two methods are implemented in a new algorithm with a sophisticated filtering strategy, which efficiently selects potential SPRs and concentrates most of the likelihood computation on the promising moves.

RESULTS: Experiments with real datasets comprising 35-250 taxa show that, while indeed greatly reducing the amount of computation, our approach provides likelihood values at least as good as those of the best-known ML methods so far and is very robust to poor starting trees. Furthermore, combining our new SPR algorithm with local moves such as PHYML's nearest neighbor interchanges, the time needed to find good solutions can sometimes be reduced even more.},
author = {Hordijk, Wim and Gascuel, Olivier},
doi = {10.1093/bioinformatics/bti713},
file = {:home/sharad/Documents/Mendeley Desktop/Hordijk, Gascuel - 2005 - Improving the efficiency of SPR moves in phylogenetic tree search methods based on maximum likelihood.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computational Biology,Databases, Genetic,Likelihood Functions,Mathematics,Models, Genetic,Phylogeny,SPR,Software},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
mendeley-tags = {SPR},
number = {24},
pages = {4338--47},
pmid = {16234323},
title = {{Improving the efficiency of SPR moves in phylogenetic tree search methods based on maximum likelihood.}},
url = {http://bioinformatics.oxfordjournals.org/content/21/24/4338.abstract},
volume = {21},
year = {2005}
}
@inproceedings{Hu2013,
author = {Hu, Yuening and Boyd-Graber, Jordan L. and III, Hal Daume and Ying, Z. Irene},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/sharad/Documents/Mendeley Desktop/Hu et al. - 2013 - Binary to Bushy Bayesian Hierarchical Clustering with the Beta Coalescent.pdf:pdf},
pages = {1079--1087},
title = {{Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent}},
url = {https://papers.nips.cc/paper/5161-binary-to-bushy-bayesian-hierarchical-clustering-with-the-beta-coalescent},
year = {2013}
}
@article{Hutter2004,
abstract = {Given i.i.d. data from an unknown distribution, we consider the problem of predicting future items. An adaptive way to estimate the probability density is to recursively subdivide the domain to an appropriate data-dependent granularity. A Bayesian would assign a data-independent prior probability to "subdivide", which leads to a prior over infinite(ly many) trees. We derive an exact, fast, and simple inference algorithm for such a prior, for the data evidence, the predictive distribution, the effective model dimension, and other quantities.},
archivePrefix = {arXiv},
arxivId = {math/0411515},
author = {Hutter, Marcus},
eprint = {0411515},
file = {:home/sharad/Documents/Mendeley Desktop/Hutter - 2004 - Fast Non-Parametric Bayesian Inference on Infinite Trees.pdf:pdf},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics,Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
primaryClass = {math},
title = {{Fast Non-Parametric Bayesian Inference on Infinite Trees}},
url = {http://arxiv.org/abs/math/0411515},
year = {2004}
}
@article{Kingman1982,
abstract = {The n-coalescent is a continuous-time Markov chain on a finite set of states, which describes the family relationships among a sample of n members drawn from a large haploid population. Its transition probabilities can be calculated from a factorization of the chain into two independent components, a pure death process and a discrete-time jump chain. For a deeper study, it is useful to construct a more complicated Markov process in which n-coalescents for all values of n are embedded in a natural way.},
author = {Kingman, J.F.C.},
doi = {10.1016/0304-4149(82)90011-4},
file = {::},
issn = {03044149},
journal = {Stochastic Processes and their Applications},
keywords = {Genetical models,Markov process,coupling,exchangeability,haploid genealogy,jump chain,random equivalent relations},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
title = {{The coalescent}},
url = {http://www.sciencedirect.com/science/article/pii/0304414982900114},
year = {1982}
}
@article{Knowles2011,
abstract = {We introduce the Pitman Yor Diffusion Tree (PYDT) for hierarchical clustering, a generalization of the Dirichlet Diffusion Tree (Neal, 2001) which removes the restriction to binary branching structure. The generative process is described and shown to result in an exchangeable distribution over data points. We prove some theoretical properties of the model and then present two inference methods: a collapsed MCMC sampler which allows us to model uncertainty over tree structures, and a computationally efficient greedy Bayesian EM search algorithm. Both algorithms use message passing on the tree structure. The utility of the model and algorithms is demonstrated on synthetic and real world data, both continuous and binary.},
archivePrefix = {arXiv},
arxivId = {1106.2494},
author = {Knowles, David A. and Ghahramani, Zoubin},
eprint = {1106.2494},
file = {:home/sharad/Documents/Mendeley Desktop/Knowles, Ghahramani - 2011 - Pitman-Yor Diffusion Trees.pdf:pdf},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
title = {{Pitman-Yor Diffusion Trees}},
url = {http://arxiv.org/abs/1106.2494},
year = {2011}
}
@article{Knowles2015,
abstract = {In this paper we introduce the Pitman Yor Diffusion Tree (PYDT), a Bayesian non-parametric prior over tree structures which generalises the Dirichlet Diffusion Tree [Neal, 2001] and removes the restriction to binary branching structure. The generative process is described and shown to result in an exchangeable distribution over data points. We prove some theoretical properties of the model including showing its construction as the continuum limit of a nested Chinese restaurant process model. We then present two alternative MCMC samplers which allows us to model uncertainty over tree structures, and a computationally efficient greedy Bayesian EM search algorithm. Both algorithms use message passing on the tree structure. The utility of the model and algorithms is demonstrated on synthetic and real world data, both continuous and binary.},
author = {Knowles, David A. and Ghahramani, Zoubin},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
publisher = {IEEE Computer Society},
title = {{Pitman Yor Diffusion Trees for Bayesian Hierarchical Clustering}},
year = {2015}
}
@article{Mauldin1992,
author = {Mauldin, R. Daniel and Sudderth, William D. and Williams, S. C.},
file = {:home/sharad/Documents/Mendeley Desktop/Mauldin, Sudderth, Williams - 1992 - Polya Trees and Random Distributions.pdf:pdf},
issn = {2168-8966},
journal = {The Annals of Statistics},
keywords = {Derechlet distributions,Polya urns,Prior distributions,random measures},
language = {EN},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics,Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
month = {sep},
number = {3},
pages = {1203--1221},
publisher = {Institute of Mathematical Statistics},
title = {{Polya Trees and Random Distributions}},
url = {http://projecteuclid.org/euclid.aos/1176348766},
volume = {20},
year = {1992}
}
@article{McCullagh2008,
author = {McCullagh, Peter and Pitman, Jim and Winkel, Matthias},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {Aldous' beta-splitting model,Gibbs distribution,Markov branching model,Poissonâ€“Dirichlet distribution},
language = {EN},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
title = {{Gibbs fragmentation trees}},
url = {http://projecteuclid.org/euclid.bj/1225980568},
year = {2008}
}
@inproceedings{Meeds2008,
author = {Meeds, Edward W. and Ross, David A. and Zemel, Richard S. and Roweis, Sam T.},
booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2008.4587559},
file = {:home/sharad/Documents/Mendeley Desktop/Meeds et al. - 2008 - Learning stick-figure models using nonparametric Bayesian priors over trees.pdf:pdf},
isbn = {978-1-4244-2242-5},
issn = {1063-6919},
keywords = {Bayes methods,Bayesian methods,Computer science,Humans,Inference algorithms,Joints,Kinematics,Motion analysis,Robustness,Shape,Uncertainty,computer animation,explanation,image motion analysis,inference procedures,learning (artificial intelligence),motion-capture data,multiple explanation learning,nonparametric Bayesian distribution,nonparametric statistics,probabilistic stick-figure model,statistical distributions,tree nodes,trees (mathematics)},
language = {English},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics,Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
month = {jun},
pages = {1--8},
publisher = {IEEE},
title = {{Learning stick-figure models using nonparametric Bayesian priors over trees}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4587559},
year = {2008}
}
@article{Neal2001,
author = {Neal, Radford M.},
file = {:home/sharad/Documents/Mendeley Desktop/Neal - 2001 - Defining priors for distributions using Dirichlet diffusion trees.pdf:pdf},
journal = {Technical Report 0104, Department},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
title = {{Defining priors for distributions using Dirichlet diffusion trees}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Defining+Priors+for+Distributions+Using+Dirichlet+Diffusion+Trees{\#}0},
volume = {01},
year = {2001}
}
@article{Neal2003,
author = {Neal, Radford M.},
file = {:home/sharad/Documents/Mendeley Desktop/Neal - 2003 - Density modeling and clustering using Dirichlet diffusion trees(4).pdf:pdf},
journal = {Bayesian Statistics},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics,Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
title = {{Density modeling and clustering using Dirichlet diffusion trees}},
url = {http://people.ee.duke.edu/{~}lcarin/Ivo10.26.07.pdf},
year = {2003}
}
@article{Paisley2014,
abstract = {We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP generalizes the nested Chinese restaurant process (nCRP) to allow each word to follow its own path to a topic node according to a document-specific distribution on a shared tree. This alleviates the rigid, single-path formulation assumed by the nCRP, allowing documents to easily express complex thematic borrowings. We derive a stochastic variational inference algorithm for the model, which enables efficient inference for massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 2.7 million documents from Wikipedia.},
archivePrefix = {arXiv},
arxivId = {1210.6738},
author = {Paisley, John and Wang, Chong and Blei, David and Jordan, Michael},
doi = {10.1109/TPAMI.2014.2318728},
eprint = {1210.6738},
isbn = {0162-8828 VO  - 37},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {99},
pages = {1--1},
pmid = {26353240},
title = {{Nested Hierarchical Dirichlet Processes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6802355},
volume = {PP},
year = {2014}
}
@article{Pitman1999,
author = {Pitman, Jim},
issn = {2168-894X},
journal = {The Annals of Probability},
keywords = {Ewens sampling formula,Exchangeable random partition,coagulation,fragmentation,random discrete distribution,ranked frequencies,stable subordinator,time reversal,two-parameter Poisson â€“Dirichlet},
month = {oct},
number = {4},
pages = {1870--1902},
publisher = {Institute of Mathematical Statistics},
title = {{Coalescents With Multiple Collisions}},
url = {http://projecteuclid.org/euclid.aop/1022874819},
volume = {27},
year = {1999}
}
@article{Spade2013,
author = {Spade, DA},
file = {:home/sharad/Documents/Mendeley Desktop/DaveSpade{\_}Thesis.pdf:pdf},
keywords = {SPR},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
mendeley-tags = {SPR},
title = {{Investigating Convergence of Markov Chain Monte Carlo Methods for Bayesian Phylogenetic Inference}},
url = {https://etd.ohiolink.edu/!etd.send{\_}file?accession=osu1372173121{\&}disposition=attachment},
year = {2013}
}
@article{Teh2009,
abstract = {We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman's coalescent. We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We show experimentally the superiority of our algorithms over others, and demonstrate our approach in document clustering and phylolinguistics.},
archivePrefix = {arXiv},
arxivId = {0907.0781},
author = {Teh, Yee Whye and Daum{\'{e}}, Hal and Roy, Daniel},
eprint = {0907.0781},
file = {:home/sharad/Documents/Mendeley Desktop/Teh, Daum{\'{e}}, Roy - 2009 - Bayesian Agglomerative Clustering with Coalescents.0781:0781},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics,Computer Science/Artificial Intelligence/Machine Learning {\&} Statistics/Bayesian Nonparametrics/Tree Priors},
title = {{Bayesian Agglomerative Clustering with Coalescents}},
url = {http://arxiv.org/abs/0907.0781},
year = {2009}
}
@inproceedings{Teh2011,
author = {Teh, Yee W. and Blundell, Charles and Elliott, Lloyd},
booktitle = {Advances in Neural Information Processing Systems},
pages = {819--827},
title = {{Modelling Genetic Variations using Fragmentation-Coagulation Processes}},
url = {https://papers.nips.cc/paper/4211-modelling-genetic-variations-using-fragmentation-coagulation-processes},
year = {2011}
}
@article{Shi2000,
abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph },
archivePrefix = {arXiv},
arxivId = {cs/0703101v1},
author = {Shi, J. and Malik, J.},
doi = {10.1109/34.868688},
eprint = {0703101v1},
isbn = {0818678224},
issn = {0162-8828},
journal = {Ieee Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {888--905},
pmid = {15742889},
primaryClass = {cs},
title = {{Normalized Cuts and Image Segmentation}},
url = {http://www.computer.org/portal/web/csdl/doi?doc=abs/proceedings/cvpr/1997/7822/00/78220731abs.htm$\backslash$npapers3://publication/uuid/268FC197-AF47-4C7C-887F-BEDB94A81320},
volume = {22},
year = {2000}
}
@article{Swofford1990,
abstract = {Useful instructions on how to do things like neighbour joining, UPGMA, parsimony. Really good discussion too},
author = {Swofford, D L and Olsen, G J},
journal = {Molecular Systematics},
number = {11},
pages = {411--501},
title = {{Phylogeny reconstruction}},
volume = {chap 11},
year = {1990}
}
@inproceedings{Vikram2016,
abstract = {Clustering is a powerful tool in data analysis, but it is often difficult to find a grouping that aligns with a user's needs. To address this, several methods incorporate constraints obtained from users into clustering algorithms, but unfortunately do not apply to hierarchical clustering. We design an interactive Bayesian algorithm that incorporates user interaction into hierarchical clustering while still utilizing the geometry of the data by sampling a constrained posterior distribution over hierarchies. We also suggest several ways to intelligently query a user. The algorithm, along with the querying schemes, shows promising results on real data.},
archivePrefix = {arXiv},
arxivId = {1602.03258},
author = {Vikram, Sharad and Dasgupta, Sanjoy},
eprint = {1602.03258},
title = {{Interactive Bayesian Hierarchical Clustering}},
url = {http://arxiv.org/abs/1602.03258},
year = {2016}
}
@article{VonLuxburg2007,
abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
author = {{Von Luxburg}, Ulrike},
journal = {Statistics and Computing},
keywords = {Graph Laplacian,Spectral clustering},
title = {{A tutorial on spectral clustering}},
year = {2007}
}
